import optuna
import pandas as pd
import numpy as np
import os
import torch
import torch.nn as nn

# --- Library Imports ---
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor
from sb3_contrib import RecurrentPPO # We use the Recurrent (LSTM) PPO
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement
from stable_baselines3.common.monitor import Monitor

# --- Local Imports ---
import dataloading
from trading_env import TradingEnv

# --- Constants ---
TRAIN_FILE = "data/processed_train.csv"
VAL_FILE = "data/processed_val.csv"

def load_data_for_tuning():
    """
    Loads raw data and processed features, then aligns them exactly by index.
    """
    print("--- Loading Data for Tuning ---")
    
    # 1. Load Raw Data (Prices) using the NEW function
    df_full = dataloading.get_full_dataset()
    
    # 2. Slice Raw Data to match our Regime (Train < 2024, Val = 2024)
    VAL_START = pd.Timestamp("2024-01-01", tz="UTC")
    TEST_START = pd.Timestamp("2025-01-01", tz="UTC")
    
    df_raw_train = df_full[df_full.index < VAL_START].copy()
    df_raw_val = df_full[(df_full.index >= VAL_START) & (df_full.index < TEST_START)].copy()
    
    # 3. Load Processed Features (Generated by main.py)
    if not os.path.exists(TRAIN_FILE) or not os.path.exists(VAL_FILE):
        raise FileNotFoundError("Processed data not found! Run main.py first.")
        
    df_features_train = pd.read_csv(TRAIN_FILE, index_col=0, parse_dates=True)
    df_features_val = pd.read_csv(VAL_FILE, index_col=0, parse_dates=True)
    
    # 4. ALIGNMENT (Crucial Step)
    # Ensure raw prices and features have exactly the same rows
    common_train = df_raw_train.index.intersection(df_features_train.index)
    df_raw_train = df_raw_train.loc[common_train]
    df_features_train = df_features_train.loc[common_train]
    
    common_val = df_raw_val.index.intersection(df_features_val.index)
    df_raw_val = df_raw_val.loc[common_val]
    df_features_val = df_features_val.loc[common_val]
    
    print(f"Training Data: {len(df_features_train)} rows")
    print(f"Validation Data: {len(df_features_val)} rows")
    
    return df_features_train, df_raw_train, df_features_val, df_raw_val

# Load data once globally to save time during tuning
DF_FEAT_TRAIN, DF_RAW_TRAIN, DF_FEAT_VAL, DF_RAW_VAL = load_data_for_tuning()

def objective(trial):
    """
    Optuna Objective Function:
    Suggests hyperparameters -> Trains Agent -> Returns Validation Reward
    """
    
    # --- 1. Suggest Hyperparameters ---
    learning_rate = trial.suggest_float("learning_rate", 1e-5, 1e-3, log=True)
    gamma = trial.suggest_float("gamma", 0.90, 0.9999)
    gae_lambda = trial.suggest_float("gae_lambda", 0.90, 1.0)
    ent_coef = trial.suggest_float("ent_coef", 0.00001, 0.01, log=True)
    max_grad_norm = trial.suggest_float("max_grad_norm", 0.3, 1.0)
    
    # LSTM Specifics
    n_steps = trial.suggest_categorical("n_steps", [128, 256, 512, 1024, 2048])
    batch_size = trial.suggest_categorical("batch_size", [64, 128, 256])
    lstm_hidden_size = trial.suggest_categorical("lstm_hidden", [64, 128, 256])
    
    # Constraint: Batch size must be a factor of n_steps (or smaller)
    if batch_size > n_steps:
        batch_size = n_steps

    # --- 2. Setup Environments ---
    # We use a simple DummyVecEnv for tuning speed
    train_env = DummyVecEnv([lambda: TradingEnv(DF_FEAT_TRAIN, DF_RAW_TRAIN)])
    val_env = DummyVecEnv([lambda: Monitor(TradingEnv(DF_FEAT_VAL, DF_RAW_VAL))])
    
    # --- 3. Define Model (RecurrentPPO) ---
    model = RecurrentPPO(
        "MlpLstmPolicy",
        train_env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        gamma=gamma,
        gae_lambda=gae_lambda,
        ent_coef=ent_coef,
        max_grad_norm=max_grad_norm,
        policy_kwargs=dict(
            enable_critic_lstm=True,
            lstm_hidden_size=lstm_hidden_size,
            net_arch=dict(pi=[64, 64], vf=[64, 64]),
            activation_fn=nn.Tanh
        ),
        verbose=0,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    

    
    
    # --- 4. Train with Early Stopping ---
    # We train for a shorter period during tuning to save time
    
    eval_freq = 10000  # <--- Change this from n_steps to 5000
    eval_freq = max(eval_freq, n_steps)
    
    eval_callback = EvalCallback(
        val_env, 
        best_model_save_path=None,
        log_path=None, 
        eval_freq=eval_freq,
        deterministic=True, 
        render=False
    )
    
    try:
        # Train for 150,000 steps (enough to see if it learns)
        model.learn(total_timesteps=150000, callback=eval_callback)
    except Exception as e:
        print(f"Trial failed with error: {e}")
        return -1000 # Return bad score on crash
        
    # --- 5. Evaluate Performance ---
    # We use Sharpe Ratio (or Mean Reward) on Validation Set
    mean_reward, std_reward = evaluate_policy(model, val_env, n_eval_episodes=5)
    
    return mean_reward

def run_tuning():
    print("--- Starting Optuna Tuning ---")
    
    # Create study to MAXIMIZE reward
    study = optuna.create_study(direction="maximize")
    
    # Run 30 trials (increase this if you have time, e.g., 50 or 100)
    study.optimize(objective, n_trials=20, show_progress_bar=True)
    
    print("\n--- Tuning Complete ---")
    print("Best Params:", study.best_params)
    print("Best Value:", study.best_value)
    
    # Save best params to file so trade.py can read them
    with open("best_hyperparams.txt", "w") as f:
        f.write(str(study.best_params))
        
if __name__ == "__main__":
    run_tuning()